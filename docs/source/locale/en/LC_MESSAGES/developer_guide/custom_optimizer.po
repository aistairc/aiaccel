# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, AIST
# This file is distributed under the same license as the aiaccel package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: aiaccel \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-15 12:16+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"

#: ../../source/developer_guide/custom_optimizer.md:1
msgid "カスタムオプティマイザー作成ガイド"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:3
msgid ""
"オプティマイザは，ハイパーパラメータを生成する機能を担うためどのハイパーパラメータの組を選択するかのアルゴリズムにより，最適化のパフォーマンスは大きく変わってきます．"
" 本稿では，ユーザーがaiaccelを用いた独自のオプティマイザを開発するための方法について解説します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:6
msgid "カスタムオプティマイザの実行確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:8
msgid ""
"それでは，まずカスタムオプティマイザを実行するための動作を確かめてみましょう． 所謂Hello, Worldの位置づけです． "
"ここで解説する内容は簡単にまとめると下記のようになります．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:12
msgid "カスタムオプティマイザのソースファイルを作成する"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:13
msgid "カスタムオプティマイザのソースファイルをaiaccelが読み込めるようにする"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:14
msgid "コンフィグレーションファイルを編集し，カスタムオプティマイザを実行できるようにする"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:16
msgid ""
"カスタムオプティマイザをどのように実行するかは環境に依存しますが，今回は自分で作成したワークスペースディレクトリに追加して実行してみます． "
"その他にはソースコードに追加するなど，環境次第でやり方は異なりますので各自の環境に合わせて設定してください．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:19
msgid "開発環境の確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:21
msgid ""
"まず環境の確認からします． 本ガイドでは，各種インストールは終了しaiaccelが実行できる状態から始めます． "
"インストールが未だの方はインストールガイドを参照してください． 各ディレクトリ・ファイルは以下のとおりとします． "
"皆さんの環境に読み替えて参考にしてください．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:27
msgid "aiaccelのソースディレクトリ: /workspace/aiaccel"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:28
msgid "ワークスペースディレクトリ: /workspace/aiaccel/work"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:29
msgid ""
"例えばランダムオプティマイザのファイル: "
"/workspace/aiaccel/aiaccel/optimizer/random_optimizer.py"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:31
msgid "カスタムオプティマイザファイルの作成"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:33
msgid "カスタムオプティマイザのソースファイルを作成します． 今回はランダムオプティマイザをコピーします．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:46
msgid "これでcustom_optimizer.pyが作成されました．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:48
msgid "ファイルの編集"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:50
msgid "このままではcustom_optimizer.pyの内容はランダムオプティマイザと同じですので，少しだけ編集します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:52
#: ../../source/developer_guide/custom_optimizer.md:189
#: ../../source/developer_guide/custom_optimizer.md:249
#: ../../source/developer_guide/custom_optimizer.md:263
#: ../../source/developer_guide/custom_optimizer.md:300
#: ../../source/developer_guide/custom_optimizer.md:344
#: ../../source/developer_guide/custom_optimizer.md:366
msgid "***/workspace/aiaccel/work/lib/my_optimizer/custom_optimizer.py***"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:66
msgid ""
"上記はdiff表記で削除された行頭に - 追加された行頭に + が付いています． "
"クラス名をRandomOptimizerからCustomOptimizerに変更しました． 変更したファイルは保存します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:70
msgid "パスの設定"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:72
msgid ""
"今custom_optimizer.pyにはランダムオプティマイザを全く同じ動作をするCustomOptimizerを定義しました． "
"aiaccelから作成したCustomOptimizerクラスを実行する必要があるので，まず__init__.pyファイルを作成します． "
"ここではaiaccel/optimizerにある__init__.pyを編集して利用します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:84
msgid "追加したCustomOptimizerを読み込むように編集します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:86
msgid "***/workspace/aiaccel/work/lib/my_optimizer/__init__.py***"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:107
msgid "次にパスの設定を行います． PYTHONPATHに，aiaccelと追加したcustom_optimizer.pyのディレクトリを追加します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:120
msgid "ユーザーファイルの作成"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:122
msgid ""
"カスタムオプティマイザを作成したので，実際に実行するユーザーファイルを作成します． "
"今回は/workspace/aiaccel/examples/sphereディレクトリをコピーして作成します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:141
msgid ""
"examples/sphereディレクトリをコピーし，sphereディレクトリに移動しました． 次にコンフィグレーションファイルを編集します． "
"オプティマイザに今回作成したカスタムオプティマイザを利用したいのでconfig.yamlを編集します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:145
#: ../../source/developer_guide/custom_optimizer.md:329
msgid "***/workspace/aiaccel/work/sphere/config.yaml***"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:157
msgid "これでコンフィグレーションファイルの編集は一旦終了です． 編集したファイルを保存します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:160
msgid "実行の確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:162
#: ../../source/developer_guide/custom_optimizer.md:308
#: ../../source/developer_guide/custom_optimizer.md:412
msgid "それでは現在のディレクトリで実行してみます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:172
msgid "正常に実行できれば成功です． このカスタムオプティマイザの中身はランダムオプティマイザと同じなので，ランダムにハイパーパラメータが選択されます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:175
msgid "ここまででカスタムオプティマイザが実行できることが確認できたので，次節ではオプティマイザのソースコードを編集して実行する点を解説します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:177
msgid "カスタムオプティマイザの編集"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:179
msgid ""
"前節でカスタムオプティマイザの実行確認を行いました． 本節では，前節で作成したカスタムオプティマイザを編集しシンプルなアルゴリズムを実装します． "
"簡単のため前節で作成したワークスペースを流用し５つのfloat型のハイパーパラメータに対し正規分布でハイパーパラメータを生成するオプティマイザを作成してみましょう．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:183
msgid "既存のオプティマイザには，ランダム・Sobol’列・グリッド・NelderMead・TPEがサポートされていますが，ランダムオプティマイザをコピーしたソースファイルから編集を始めます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:185
msgid "ランダムオプティマイザの確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:187
msgid "前節でコピーしたカスタムオプティマイザのファイルを見てみましょう．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:222
msgid ""
"`CustomOptimizer` クラスは` AbstractOptimizer` を継承し，`generate_parameter` "
"メソッドのみを実装しています． `generate_parameter` 以外のオプティマイザとしての機能は "
"`AbstractOptimizer` に実装されているので，`generate_parameter` "
"メソッドを実装すれば簡単なオプティマイザなら実装することができます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:225
msgid "ハイパーパラメータのソースコードの確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:227
msgid ""
"`generate_parameter` メソッドを見てみましょう． `self.params.sample` というメソッドを実行しています． "
"このメソッドは，aiaccel/parameter.py の `HyperParameterConfiguration` インスタンスである "
"`self.params` の `sample` メソッドです． 引数として与えられている `self._rng` は 継承元の "
"`AbstractOptimizer` が持つ `numpy.random.RandomState` オブジェクトです． `sample` "
"メソッド内では，さらに `HyperParameter` インスタンスである `value` から更に `sample` "
"メソッドが呼ばれています． この２度目に呼ばれた `sample` メソッドは `HyperParameter` "
"クラスのメソッドであり，中身を見てみるとハイパーパラメータのタイプごとに処理が分かれていますが，例えば `FLOAT` 型の場合 "
"`numpy.random.RandomState.uniform` が実行されます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:234
msgid "***/workspace/aiaccel/aiaccel/parameter.py***"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:241
msgid "こうして生成されたランダムなハイパーパラメータを返すことが `generate_parameter` メソッドの役割となります．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:243
msgid "正規分布オプティマイザの作成"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:245
msgid ""
"では，aiaccel/parameter.py の `HyperParameterConfiguration` "
"クラスをもう少し詳しく見てみましょう． `sample` メソッドの他に，`get_parameter_list` というメソッドがあります． "
"このメソッドは，`sample` メソッドでハイパーパラメータをランダムに選択する前のハイパーパラメータのリストを返します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:260
msgid ""
"次に正規分布を用いてハイパーパラメータを生成します． aiaccel/parameter.py の `HyperParameter` クラスでは "
"numpy の `random.RandomState.uniform` を実行していましたが，今回は正規分布なので numpy の "
"`random.RandomState.normal` を利用します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:298
msgid "正規分布で生成したハイパーパラメータが，最大値・最小値を超えないよう修正を加えています．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:306
#: ../../source/developer_guide/custom_optimizer.md:410
msgid "正規分布オプティマイザの実行確認"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:318
msgid "正常に終了すれば成功です．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:320
msgid "オプティマイザへの変数の導入"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:322
msgid ""
"正規分布のオプティマイザの平均と分散の値はハードコーディングしていました． "
"このようなオプティマイザに利用する変数はコンフィグレーションファイルや引数として渡したい値です．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:325
msgid "ここでは平均と分散をコンフィグレーションファイルから与える方法について解説します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:327
msgid "まずコンフィグレーションファイルに以下の追加をします．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:341
msgid "mu と sigma が追加されました． 次に custom_optimizer.py を編集して，mu と sigma を取得できるようにします．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:363
msgid ""
"__init__ メソッドを追加し，コンフィグレーションから mu と sigma を取得し変数として保持しました． あとは "
"`self._rng.normal` を呼ぶ際に mu と sigma を渡します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:422
msgid ""
"/workspace/aiaccel/work/sphere/workディレクトリに実行結果が保存されます． "
"前回事項した結果と異なることを確認してみてください．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:425
msgid "オプティマイザ内部からの目的関数の値の参照"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:427
msgid ""
"この節では，オプティマイザの内部から過去に計算した目的関数の値を参照して，次のパラメータを決定する方法を確認します． aiaccel 上のジョブの"
" ID が  `n` のときの目的関数の値は，`AbstractOptimizer` "
"を継承して作成したカスタムオプティマイザの内部から，次のようにして取得できます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:432
msgid ""
"この処理の後，`objective_value` は `n` で指定した目的関数の値か `None` を保持します． `None` "
"が保持されるのは，`self.storage.result.get_any_trial_objective()` "
"が呼ばれた時点で，目的関数の値を計算する user program が Storage に 計算結果を保存していない場合です．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:435
msgid "例: 勾配降下法による最適化"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:437
msgid ""
"勾配降下法では，着目する試行におけるパラメータが与える目的関数の勾配を元に，次のパラメータを決定します． 一次元の場合， $n$ "
"試行目のパラメータを $W_n$ とし，目的関数を $f(W_n)$ と書くと， $n+1$ 試行目のパラメータは"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:440
msgid " W_{n+1} = W_n + \\gamma f'(W_{n}) "
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:442
msgid ""
"となります．ここで， $\\gamma$ は学習率 (パラメータの更新をどの程度行うかの指標)， $f'(W_n)$ は $W_n$ "
"における目的関数の勾配です．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:445
msgid ""
"ここでは $f$ の解析的な形が分からない場合に，勾配を差分で置き換えることを考えます． "
"簡単のため，前進差分のみを考えると，差分を用いた勾配の近似式は"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:448
msgid "f'(W_n) \\approx \\frac{f(W_n + \\delta) - f(W_n) } { \\delta }"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:450
msgid "となります． 従って $n + 1$ 試行目におけるパラメータは"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:453
msgid ""
"W_{n + 1} \\approx W_n + \\gamma \\frac{f(W_n + \\delta) - f(W_n) } { "
"\\delta }"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:455
msgid "と近似できます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:457
msgid "オプティマイザの実装"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:459
msgid ""
"上の例では $n + 1$ 試行目のパラメータを決定するために， $f(W_n)$ と $f(W_{n+1})$ という 2 "
"つの目的関数の値を使用しました． カスタムオプティマイザでは，これらをメソッド `generate_parameter()` "
"内で取得する必要があります． 以下に，前進差分を用いたオプティマイザの例を示します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:568
msgid "このオプティマイザは以下で説明する 4 つの状態を取ります．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:569
msgid "PREPARE: オプティマイザが保持する変数やリストの初期化を行います．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:570
msgid "CALC_FORWARD: $W_n + \\delta$ を計算します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:571
msgid "WAIT_CURRENT_OBJECTIVE: Storage に $W_n$ のときの目的関数の値が保存されるまで待機します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:572
msgid "WAIT_FORWARD_OBJECTIVE: Storage に $W_n + \\delta$ のときの目的関数の値が保存されるまで待機します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:573
msgid "CALC_NEXT_PARAM: $n+1$ 試行目のパラメータを計算します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:575
msgid "これらの状態を `Enum` モジュールを用いて実装しています．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:585
msgid "オプティマイザが保持する変数は以下の通りです．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:586
msgid "learning_rate: 学習率．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:587
msgid "delta: 目的関数の前進値を計算するための変分 $\\delta$."
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:588
msgid "current_params: 現在 ( $n$ 試行目) のパラメータ $W_n$．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:589
msgid "num_parameters: 最適化するパラメータの数．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:590
msgid "forward_objectives: $W_n + \\delta$ における目的関数の値 $f(W_n + \\delta)$．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:591
msgid "num_generated_forwards: 既に生成された $W_n + \\delta$ の数．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:592
msgid "num_calculated_forward_objectives: 計算が完了した $f(W_n + \\delta)$ の数．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:593
msgid "forward_ids: $f(W_n + \\delta)$ の計算が実行される aiaccel 上のジョブ ID (`trial_id`)."
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:595
msgid "`generate_parameter()` 内の処理の流れ"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:597
msgid "状態: PREPARE"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:611
msgid ""
"オプティマイザが保持する変数やリストの初期化を行います． 初期化を行った後，オプティマイザの状態を `PREPARE` から "
"`CALC_FORWARD` に変更します． この状態では，メソッド `self.generate_parameters()` は必ず "
"`None` を返却します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:616
msgid "状態: CALC_FORWARD"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:635
msgid ""
"パラメータの前進値 $W_n + \\delta$ を ***1 回の呼び出しで 1 つだけ***計算し，aiaccel "
"のメインループに値を返却します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:637
msgid "まず，パラメータのリストから前進値を計算するパラメータを一つずつ取り出します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:642
msgid "ここで `self.num_generated_forwards` は，既に生成されたパラメータの前進値の総数を表します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:644
msgid "続いて，項目 `value` を $W_n + \\delta$ で置き換えます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:654
msgid "同時に，返却するパラメータの前進値を計算する aiaccel 上のジョブ ID (`trial_id`) を保持します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:658
msgid "`self.current_id` は，この***カスタムオプティマイザ内で***以下のように定義されるプロパティです．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:665
msgid "計算された前進値の数 `self.num_generated_forwards` をインクリメントします．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:670
msgid ""
"全てのパラメータについて，その前進値が計算されたとき，オプティマイザの状態を `CALC_FORWARD` から "
"`WAIT_CURRENT_OBJECTIVE` に変更します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:677
msgid ""
"注意: オプティマイザの状態が `CALC_FORWARD` のとき，メソッド `generate_parameters()` "
"は最適化するパラメータの数と同じ回数だけ aiaccel のメインループに呼ばれます．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:680
msgid "状態: WAIT_CURRENT_OBJECTIVE"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:690
msgid "現在のパラメータ $W_n$ における目的関数の値 $f(W_n)$ が Storage に保存されるまで待ち，その値を取得します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:694
msgid "`self._get_objective()` は***カスタムオプティマイザ内***で以下のように定義されるメソッドです．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:699
msgid ""
"このメソッドは，取得したい目的関数の値の `trial_id` (aiaccel 上のジョブ ID) を引数に取り，Storage "
"から値を読み出します． ただし，呼び出された時点で Storage に値が保存されていなければ，`None` を返却します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:702
msgid ""
"メソッド `self._get_objective()` が目的関数の値を返した場合，オプティマイザの状態を "
"`WAIT_CURRENT_OBJECTIVE` から `WAIR_FORWARD_OBJECTIVE` に変更します． この時点で， "
"$f(W_n)$ の値はメンバ変数 `self.current_objective` に保持されています．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:709
msgid ""
"注意: オプティマイザが状態 `WAIT_CURRENT_OBJECTIVE` のとき，メソッド "
"`self.generate_parameters()` は 1 回以上，Storage に対象とする目的関数の値が保存されるまで呼ばれます． "
"また，Storage から目的関数の値を読み出せたか否かに関わらず，`WAIT_CURRENT_OBJECTIVE` 状態の "
"`self.generate_parameters()` は `None` をメインループに返します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:712
msgid "状態: WAIT_FORWARD_OBJECTIVE"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:728
msgid ""
"パラメータの前進値 $W_{n} + \\delta$ における目的関数の値 $f(W_n + \\delta)$ が Storage "
"に保存されるのを待ち，その値を取得します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:730
msgid "まず，取得する目的関数の aiaccel 上のジョブ ID (`trial_id`) をリストから読み出します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:735
msgid "ここで，`self.num_calculated_forward_objectives` は取得済みの前進値に対する目的関数の値の総数です．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:737
msgid "続いて，メソッド `self._get_objective()` に読み出した ID を渡して，Storage から目的関数の値を読み出します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:742
msgid ""
"正常な値が返却された場合，返却された値をリストに保持し，取得済みの値の総数 "
"`self.num_calculated_forward_objectives` をインクリメントします．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:749
msgid ""
"このとき，すべての目的関数の値が取得できていれば，オプティマイザの状態を `WAIT_FORWARD_OBJECTIVE` から "
"`CALC_NEXT_PARAM` に変更します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:755
msgid ""
"注意: オプティマイザが状態 `WAIT_FORWARD_OBJECTIVE` のとき，メソッド "
"`self.generate_parameters()` は少なくとも最適化対象のパラメータの総数以上，Storage "
"にすべての目的関数の値が保存されるまで呼ばれます．また，Storage "
"から目的関数の値が読み出せたか否かや，すべての値の読み出しが完了したか否かに依らず，`WAIT_FORWARD_OBJECTIVE` 状態の "
"`self.generate_parameters()` は `None` をメインループに返します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:757
msgid "状態: CALC_NEXT_PARAM"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:779
msgid ""
"Storage から読み出した $W_n$ における目的関数の値 $f(W_n)$ (`self.current_params`) と "
"$W_{n+1}$ における目的関数の値 $f(W_{n+1})$ (`self.forward_objectives`) "
"を用いて勾配を計算します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:788
msgid "計算した勾配を用いて次のパラメータ $W_{n+1}$ を計算して `dict` 型オブジェクトを作成し，リストに保持します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:799
msgid "オプティマイザの状態を `CALC_NEXT_PARAM` から `PREPARE` に変更し，作成した次のパラメータをメインループに返却します．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:806
msgid "注意事項"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:808
msgid ""
"一般に，パラメータの更新ステップ数 $n$ と aiaccel 上のジョブ ID (`trial_id`) は一致しないことに注意してください． "
"例えば上の例において，最適化したいパラメータの数が 5 個の場合，パラメータを１度更新するために目的関数を 5 回計算する必要があります． "
"この場合は 1 回のパラメータ更新で aiaccel の `trial_id` は 5 増加することになります． 従って，config.yaml "
"で指定した `trial_number` が，例えば 30 回の場合，初期値を除いて 4 回しかパラメータは更新されません．"
msgstr ""

#: ../../source/developer_guide/custom_optimizer.md:813
msgid ""
"同様な ID の不一致は NelderMeadOptimizer を用いた際にも起こります． Nelder-Mead 法の 1 "
"ステップに相当する処理が終了したとき，aiaccel 上では `trial_id` が **パラメータ数 + 1** だけ増加します．"
msgstr ""

#~ msgid "デフォルトのconfig.yamlファイルにはネルダーミードの初期値がリストで設定されているため，これは削除します．"
#~ msgstr ""

#~ msgid "既存のオプティマイザには，ランダム・ソボル列・グリッド・ネルダーミード・TPEがサポートされていますが，ランダムオプティマイザをコピーしたソースファイルから編集を始めます．"
#~ msgstr ""

#~ msgid ""
#~ "CustomOptimizerクラスはAbstractOptimizerを継承し，generate_parameterメソッドのみを実装しています．"
#~ " "
#~ "generate_parameter以外のオプティマイザとしての機能はAbstractOptimizerに実装されているので，generate_parameterメソッドを実装すれば簡単なオプティマイザなら実装することができます．"
#~ msgstr ""

#~ msgid ""
#~ "generate_parameterメソッドを見てみましょう． "
#~ "self.params.sampleというメソッドを実行しています． "
#~ "このメソッドは，aiaccel/parameter.pyのHyperParameterConfigurationインスタンスであるself.paramsのsampleメソッドです．"
#~ " "
#~ "sampleメソッド内では，さらにHyperParameterインスタンスであるvalueから更にsampleメソッドが呼ばれています．"
#~ " "
#~ "この２度目に呼ばれたsampleメソッドはHyperParameterクラスのメソッドであり，中身を見てみるとハイパーパラメータのタイプごとに処理が分かれていますが，例えばFLOAT型の場合np.random.uniformが実行されます．"
#~ msgstr ""

#~ msgid "こうして生成されたランダムなハイパーパラメータを返すことがgenerate_parameterメソッドの役割となります．"
#~ msgstr ""

#~ msgid ""
#~ "ではaiaccel/parameter.pyのHyperParameterConfigurationクラスをもう少し詳しく見てみましょう．"
#~ " sampleメソッドの他に，get_parameter_listというメソッドがあります． "
#~ "このメソッドは，sampleメソッドでハイパーパラメータをランダムに選択する前のハイパーパラメータのリストを返します．"
#~ msgstr ""

#~ msgid ""
#~ "次に正規分布を用いてハイパーパラメータを生成します． "
#~ "aiaccel/parameter.pyのHyperParameterクラスではnumpyのrandom.uniformを実行していましたが，今回は正規分布なのでnumpyのrandom.normalを利用します．"
#~ msgstr ""

#~ msgid "muとsigmaが追加されました． 次にcustom_optimizer.pyを編集して，muとsigmaを取得できるようにします．"
#~ msgstr ""

#~ msgid ""
#~ "__init__メソッドを追加し，コンフィグレーションからmuとsigmaを取得し変数として保持しました． "
#~ "あとはrandom.normalを呼ぶ際にmuとsigmaを渡します．"
#~ msgstr ""

#~ msgid "計算した勾配を用いて次のパラメータ $W_{n+1}$ を計算して `Dict` 型オブジェクトを作成し，リストに保持します．"
#~ msgstr ""

