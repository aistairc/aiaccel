walltime: "1:0:0"

script_prologue: |
    echo Job ID: $PBS_JOBID
    echo Hostname: $(hostname)

    export CUDA_VISIBLE_DEVICES=all

sbatch: "sbatch --export=USE_SSH=1"

cpu:
    sbatch_args: "-p min -N 1"
    job: "{command}"

cpu-array:
    n_tasks_per_proc: 128
    n_procs: 24
    sbatch_args: "-p cpu1 -N 1 --array=1-{args.n_tasks}:$(( {args.n_tasks_per_proc} * {args.n_procs} ))"

gpu:
    sbatch_args: "-p gpu1 -N 1"
    job: "{command}"

gpu-array:
    n_tasks_per_proc: 128
    n_procs: 8
    sbatch_args: "-p gpu1 -N 1 --array=1-{args.n_tasks}:$(( {args.n_tasks_per_proc} * {args.n_procs} ))"
    job: "CUDA_VISIBLE_DEVICES=$(( LOCAL_PROC_INDEX % 8 )) {command}"

mpi:
    n_nodes: 1
    sbatch_args: >-
        -p gpu1
        -N {args.n_nodes} --ntasks-per-node=$(( {args.n_procs} / {args.n_nodes} )) --cpus-per-task=$(( {args.n_nodes} * 96 / {args.n_procs} ))
    job: |
        source /etc/profile.d/modules.sh
        module load hpcx

        mpirun -np {args.n_procs} -bind-to none -map-by slot \\
            -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0 \\
            {command}

train:
    sbatch_args: >-
        -p $( (({args.n_gpus}==1)) && printf gpu1 || printf gpu1 )
        -N $(( ({args.n_gpus} + 7) / 8 )) --ntasks-per-node=$( (({args.n_gpus}==1)) && printf 1 || printf 8 ) --cpus-per-task=$( (({args.n_gpus}==1)) && printf 8 || printf 12 )
    job: |
        source /etc/profile.d/modules.sh
        module load hpcx

        mpirun -np {args.n_gpus} -bind-to none -map-by slot \\
            -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0 \\
            -x MAIN_ADDR=$(hostname -i) \\
            -x MAIN_PORT=3000 \\
            -x COLUMNS=120 \\
            -x PYTHONUNBUFFERED=true \\
            {command}
